# RAG LLAMA INDEX ü¶ô

<p align="center">
    <img src="https://www.peru.travel/contenido/general/imagen/en/430/1.1/portada%20llamas,%20alpacas%20y%20vicu%C3%B1as.jpg" width="500" height="400"/>
</p>

This repository hosts a full Q&A pipeline using llama index framework and vector database. The data used are Financed Emission related documents that have been extracted from different sorces. For the following pipeline all documents were used.

The main steps taken to build the RAG pipeline can be summarize as follows (a basic RAG Pipeline is performed after text cleaning):

- **Data Ingestion**: import data into the notebook

- **Text Cleaning**: replacing multiple consecutive spaces

- **Nodes**: sentence splitter and windows parser

- **Indexing:** VectorStoreIndex for indexing chunked nodes with associated service and storage contexts

- **Reranking**: re-order nodes, and returns the top N nodes

- **Scoring**: top k most similar results

Feel free to and clone this repo

## üë®‚Äçüíª **Tech Stack**

![Jupyter Notebook](https://img.shields.io/badge/jupyter-%23FA0F00.svg?style=for-the-badge&logo=jupyter&logoColor=white)
![Python](https://img.shields.io/badge/python-3670A0?style=for-the-badge&logo=python&logoColor=ffdd54)
![OpenAI](https://img.shields.io/badge/OpenAI-74aa9c?style=for-the-badge&logo=openai&logoColor=white)
![Windows](https://img.shields.io/badge/Windows-FCC624?style=for-the-badge&logo=windows&logoColor=black)
![Git](https://img.shields.io/badge/git-%23F05033.svg?style=for-the-badge&logo=git&logoColor=white)

## üìê Set Up

In the initial project phase, the documents are loaded and transformed into llama index schema documents. This allows indexing and storing them into a vector store.

Indexing is a fundamental process for storing and organizing data from diverse sources into a vector store (`VectorStoreIndex`), a structure essential for efficient storage and retrieval. This process involves storing text chunks along with their corresponding embedding representations, capturing the semantic meaning of the text (`ServiceContext` incorporates necessary configurations or services needed to generate vector representations). These embeddings facilitate easy retrieval of chunks based on their semantic similarity. Embeddings are typically generated by specialized models like BAAI/bge-small-en-v1.5 (Flag Embedding which is focused on RAG LLMs).

After indexing, a basic query retrieval is performed in order to check the Q&A functioning and performance.

## üåä RAG Pipeline

### ü™ü Nodes
--------------

When passing documents to a vector store for indexing, there are two main alternatives:

**1. Passing the Whole Document:**

- Involves indexing the entire document as a single unit.
- Suitable for smaller documents that fit comfortably within memory constraints.
- Simpler indexing process but might lack granularity in capturing diverse content within larger documents.

**2. Converting the Document into Nodes:**

- Breaks down the document into smaller, manageable chunks or nodes.
- Ideal for larger documents to prevent memory issues and for better granularity.
- Enables indexing of specific sections or segments, improving the ability to capture diverse content within the document.

As a general guideline, for larger documents, it's advantageous to break them down into smaller chunks or nodes before indexing. This approach not only helps in avoiding memory limitations but also allows for a more detailed and nuanced representation of the document's content. It facilitates better indexing granularity, potentially enhancing the retrieval and analysis of specific sections within the larger document.


- **Sentence Splitter**: allows to break down your documents into sentences. This will require a pattern to decide where a sentence starts or stop (bullet points, points,..).


### Vector Store
--------------
The vector store used to store all embeddings and metadata is **Deeplake**. It is designed for efficient storage and data handling, ensuring maximum efficiency and productivity in LLM applications. All the previously generate nodes, were store and indexed in Deeplake. 

### Reranking
--------------


- **Reranking**: after retrieval, the `SentenceTransformerRerank` uses the cross-encoders from the sentence-transformer package to re-order nodes, and returns the `top N nodes`. The crossEncoder model is a type of Sentence Transformer model that takes a pair of sentences and returns a single relevance score.

### Query Engine
--------------

Once the pipeline is set up, the query can be done. To the question `What is this document talking about?` the following output with the **score** and the top N node from where the information was retrieved.

<p align="center">
<img width="563" alt="Screen Shot 2024-01-05 at 9 05 56 AM" src="https://images.upgrad.com/e709ab65-0e76-40f5-9c30-86ac042c598c-RAG%20-%20System%20Design%202.png">
</p>

The **score** represents the relevance or similarity measure between the query and the retrieved document. This score indicates the likelihood or degree to which the document is considered relevant to the query based on the model's understanding or learned representation of the text.

For instance:

- A higher score typically suggests greater relevance or similarity between the query and the document.
- Lower scores imply lesser relevance or similarity.

### Model Evaluation

To evaluate the model Human feedback is being used and based on that Prompt can be updated for better performance.


## üìà Further Steps

This pipeline shows that a proper text preprocessing combined with a vector database, can end up in a outstanding model performance. However, there is always a path for improvement or different strategies for the data preprocessing. Some steps that can be carried out can be listed as follows:

- Different database: `FAISS`, `Chroma`, `Pinecone`,...

- Data cleaning: removal or numbers, punctuation,...

- Different model: Huggingface `BAAI/bge-reranker-base` reranker model
